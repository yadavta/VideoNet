import os, time, argparse, json, threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import timedelta
from google import genai
from tqdm import tqdm

program_start = time.time()

parser = argparse.ArgumentParser()
parser.add_argument('-b', '--benchmark', type=str, required=True, help='Local path to JSON file generated by `prep_benchmark.py`')
parser.add_argument('-t', '--tmpdir', type=str, required=True, help='Local path to tmpdir from `prep_gem.py` where local copies of clips are stored')
parser.add_argument('-o', '--outdir', type=str, required=True, help='Local path to directory where outputs will be dumped')
parser.add_argument('-n', '--batchname', type=str, required=True, help='Name of batch; will be printed in results to help organization between runs')
parser.add_argument('-k', type=int, required=True, help='Number of in-context examples being provided. Must be 0, 1, 2, or 3.')
parser.add_argument('-gfn', '--filenames', type=str, required=True, default=None, help='Optional local path to mapping from UUIDs to Gemini File API file names. Use this.')
parser.add_argument('-m', '--model', type=str, required=False, default='gemini-2.5-pro-preview-03-25', help='Name of Gemini model to use. See Google docs for the specific names. Defaults to Gemini 2.5 Pro')
parser.add_argument('--delete', required=False, action='store_true', help='Cleans up any files it uploads to Gemini File API. Not recommended.')
args = parser.parse_args()
bfile, out_dir, tmp_dir, batch_name, k, model, cache, delete = args.benchmark, args.outdir, args.tmpdir, args.batchname, args.k, args.model, args.filenames, args.delete
NUM_ATTEMPTS, SLEEP_LENGTH = 3, 10
if not os.path.isfile(bfile):
    raise Exception('ERROR: provided benchmark file could not be located.')
if not os.path.isdir(tmp_dir):
    raise Exception('ERROR: tmpdir must exist already and be populated with videos.')
if k not in set([0, 1, 2, 3]):
    raise Exception('ERROR: argument supplied via `-k` must be 0, 1, 2, or 3.')
k_dict = {0: '', 1: 'video shows', 2: 'two videos show', 3: 'three videos show'}
k_text = k_dict[k]

# load in benchmark
with open(bfile, 'r') as f:
    benchmark = json.load(f)
action_ids = list(benchmark.keys())

# get api key
api_file = os.environ.get("GOOGLE_GENAI", "/gscratch/raivn/tanush/credentials/google_genai.txt")
try:
    with open(api_file, 'r') as f:
        api_key = f.read().strip()
except Exception as e:
    print("ERROR: unable to read Google GenAI API Key at ", api_file)
    raise e

# optionally load in gfilenames
uuid_to_gfilename: dict[str, str] = {}
if cache:
    with open(cache, 'r') as f:
        uuid_to_gfilename = json.load(f)
        
# -- parallelism begins below --

# shared states
uuid_to_gfilename       # initialized above but still shared
uuid_to_gfile: dict[str, genai.types.File] = {}
results: dict[str, dict] = {}
errors: list[str] = []

uuid_dicts_lock = threading.Lock()
results_lock = threading.Lock()
errors_lock = threading.Lock()
thread_local = threading.local()

# give each thread its own Gemini client
def get_gemini_client() -> genai.Client:
    if not hasattr(thread_local, "gclient"):
        thread_local.gclient = genai.Client(api_key=api_key)
    return thread_local.gclient

# gemini file API abstraction
def upload_and_wait(uuids: list[str]):
    global uuid_to_gfile
    global uuid_to_gfilename
    for unique in uuids:
        with uuid_dicts_lock:
            if unique in uuid_to_gfile:
                continue
            if unique in uuid_to_gfilename:
                try:
                    name = uuid_to_gfilename[unique]
                    client = get_gemini_client()
                    file_obj = client.files.get(name=name)
                    uuid_to_gfile[unique] = file_obj
                    time.sleep(2)
                    continue
                except Exception as e:
                    print('exception occured', e)
                    with errors_lock:
                        errors.append(f"ERROR: unable to upload {unique} because: " + str(e))

        client = get_gemini_client()
        mp4, webm = os.path.join(tmp_dir, unique + '.mp4'), os.path.join(tmp_dir, unique + '.webm')
        if os.path.isfile(mp4): file = mp4
        elif os.path.isfile(webm): file = webm
        else: raise Exception(f'ERROR: could not locate copy of clip named {unique} in the tmpdir {tmp_dir}.')
        
        file_obj = get_gemini_client().files.upload(file=file, config={'display_name': f'{unique}'})
        print(f'Uploading clip for UUID {unique}')
        with uuid_dicts_lock:
            uuid_to_gfile[unique] = file_obj
            uuid_to_gfilename[unique] = file_obj.name
        time.sleep(SLEEP_LENGTH)

def process_id(action_id: str):
    tmp = benchmark[action_id]
    action, domain, subdomain = tmp['action'], tmp['domain'], tmp['subdomain']
    subdomain = tmp['subdomain'] if tmp['subdomain'] and tmp['subdomain'] != 'NULL' else 'action'
    a_aan = 'an' if action[0].lower() in set(['a', 'i', 'o', 'u', 'e']) else 'a'
    s_aan = 'an' if subdomain[0].lower() in set(['a', 'i', 'o', 'u', 'e']) else 'a'
    out = {}
    
    # gather in-context examples to upload
    in_context = tmp['in_context']
    upload_and_wait(in_context)
    
    # prepare general prompt
    with uuid_dicts_lock:
        in_context_gfiles = [uuid_to_gfile[u] for u in in_context]
    if k > 0:
        prompt = [f"The following {k_text} {a_aan} {action}, which is {s_aan} {subdomain} in {domain}.", *in_context_gfiles,
                  f"Now consider the following video. Is it also {a_aan} {action}? Please reason through your answer. It is critical that you output 'yes' or 'no' on the final line of your answer."]
    else:
        prompt = [f"Recall that {a_aan} {action} is {s_aan} {subdomain} in {domain}. Does the following video show {a_aan} {action}? Please reason through your answer. It is critical that you output 'yes' or 'no' on the final line of your answer."]
    
    def run_clip(clip_uuid, is_positive, i):
        client = get_gemini_client()
        with uuid_dicts_lock:
            curr_input = prompt + [uuid_to_gfile[clip_uuid]]
        for attempt_idx in range(NUM_ATTEMPTS):
            try:
                res = client.models.generate_content(
                    model=model, config=genai.types.GenerateContentConfig(temperature=0),
                    contents=curr_input
                )
                prediction = res.text.splitlines()[-1].strip().lower()
                if (prediction == 'yes' and is_positive) or (prediction == 'no' and not is_positive):
                    correct = 1
                elif prediction in {'yes', 'no'}:
                    correct = 0
                else:
                    correct = -1
                    raise Exception("Unable to parse prediction:", prediction)
                key = f'pos{i+1}' if is_positive else f'neg{i+1}'
                entry = {'accurate': correct, 'prediction': prediction, 'response': res.text}
                if not is_positive:
                    entry.update({
                        'neg_id': tmp['negatives'][i]['action_id'],
                        'neg_name': tmp['negatives'][i]['action_name']
                    })
                return key, entry
            except Exception as e:
                print('EXCEPTION: ', e)
                with errors_lock:
                    errors.append(f'ERROR: for action_id {action_id} got exception: ' + str(e))
                time.sleep(SLEEP_LENGTH + 5 * attempt_idx)
        # reaching here indicates none of the iterations were successful
        return None, f'On UUID {clip_uuid} as positive clip #{i+1} for action {action} (id: {action_id})'
    
    # gather positives and upload
    positives = tmp['positives']
    upload_and_wait(positives)
    
    # run positive clips through benchmark
    for idx, unique in enumerate(positives):
        key, value_or_error = run_clip(unique, True, idx)
        if key:
            out[key] = value_or_error
        else:
            with errors_lock:
                errors.append(value_or_error)
    
    # gather negative clips and upload
    negatives = [n['uuid'] for n in tmp['negatives']]
    upload_and_wait(negatives)
    
    # run negative clips through benchmark
    for idx, unique in enumerate(negatives):
        key, value_or_error = run_clip(unique, False, idx)
        if key:
            out[key] = value_or_error
        else:
            with errors_lock:
                errors.append(value_or_error)
    
    # save results
    with results_lock:
        results[action_id] = out

with ThreadPoolExecutor(max_workers=8) as executor:
    futures = {executor.submit(process_id, action_id): action_id for action_id in action_ids}
    for _ in tqdm(as_completed(futures), total=len(futures), desc="running benchmark"):
        pass
    
# try cleaning up on Gemini File API
if delete:
    client = genai.Client(api_key=api_key)
    deleted = 0
    for gfn in tqdm(uuid_to_gfilename.values(), desc="deleting files"):
        for _ in range(min(1,NUM_ATTEMPTS // 2)):
            try:
                client.files.delete(name=gfn)
                deleted += 1
                break
            except Exception as e:
                errors.append(f"ERROR: unable to delete file with name {gfn} from Gemini File API.")
                time.sleep(min(1, SLEEP_LENGTH // 2))

# Write raw outputs
os.makedirs(out_dir, exist_ok=True)
results_path, errors_path = os.path.join(out_dir, 'results.json'), os.path.join(out_dir, 'errors.txt')
with open(results_path, 'w') as f:
    json.dump(results, f)
with open(errors_path, 'w') as f:
    errors_txt = "\t ERRORS \t\n" + "\n".join(errors)
    f.write(errors_txt)

# Create a succint version of results for easy analysis
succint: dict[str, dict] = {}
pos_acc, neg_acc, pos_total, neg_total = 0, 0, 0, 0
for id in results.keys():
    result = results[id]
    pacc, nacc, ptotal, ntotal = 0, 0, 0, 0
    i = 1
    while f'pos{i}' in result or f'neg{i}' in result:
        if f'pos{i}' in result:
            ptotal += 1
            if result[f'pos{i}']['accurate']: pacc += 1
        if f'neg{i}' in result:
            ntotal += 1
            if result[f'neg{i}']['accurate']: nacc += 1
        i += 1
    succint[id] = {'acc': pacc + nacc, 'total': ptotal + ntotal, 
                   'pos_acc': pacc, 'pos_total': ptotal,
                   'neg_acc': nacc, 'neg_total': ntotal}
    pos_acc, neg_acc = pos_acc + pacc, neg_acc + nacc
    pos_total, neg_total = pos_total + ptotal, neg_total + ntotal
    
try:
    # Generate a text summary for even easier analysis
    acc, total = pos_acc + neg_acc, pos_total + neg_total
    pos_rate = 100 * pos_acc / pos_total if pos_total else 0.
    neg_rate = 100 * neg_acc / neg_total if neg_total else 0.
    rate = 100 * acc / total if total else 0.
    summary = f"""\n\t ######## SUMMARY ######## \t\n
    \t Positive Clips : {pos_acc} / {pos_total} correct ( {pos_rate:.2f}% )
    \t Negative Clips : {neg_acc} / {neg_total} correct ( {neg_rate:.2f}% )
    \t Total          : {acc} / {total} correct ( {rate:.2f}% )

    \t Num Errors     : {len(errors)}
    \t Num Uploads    : {len(uuid_to_gfilename.values())} files
    \t Num Deletes    : {deleted if delete else 'N/A'} files

    \t Model          : {model}
    \t Batch          : {batch_name}
    \t Date           : {time.strftime("%Y-%m-%d %H:%M:%S %z")}
    \t Length         : {timedelta(seconds=int(time.time() - program_start))}
    \n\t ######## END TRANSMISSION ######## \t\n"""
except Exception as e:
    print('\n', e, '\n')
    summary = 'an error occured'

# Write processed outputs
succint_path, summary_path = os.path.join(out_dir, 'succint.json'), os.path.join(out_dir, 'summary.txt')
gfilenames_path = os.path.join(out_dir, 'gfilenames.json')

with open(succint_path, 'w') as f:
    json.dump(succint, f)
with open(gfilenames_path, 'w') as f:
    json.dump(uuid_to_gfilename, f)
with open(summary_path, 'w') as f:
    f.write(summary)

print(summary)