import os, torch, argparse, json, time
import torchvision.transforms as T
import numpy as np

from transformers import AutoProcessor, AutoTokenizer, AutoModel
from torchvision.transforms.functional import InterpolationMode
from decord import VideoReader, cpu
from datetime import timedelta
from typing import Any
from tqdm import tqdm
from PIL import Image

NUM_ATTEMPTS = 1
SLEEP_LENGTH = 1
MODELS_DIR = os.environ.get('MODELS_DIR')
if not MODELS_DIR:
    print("ERROR: must set environemnt variable MODELS_DIR to directory where models will be loaded in. If this is on Hyak, make sure this is on gscratch and not your home directory.")
    exit(1)

program_start = time.time()
parser = argparse.ArgumentParser()
parser.add_argument('-b', '--benchmark', type=str, required=True, help='Local path to JSON file generated by `prep_benchmark.py`')
parser.add_argument('-t', '--tmpdir', type=str, required=True, help='Local path to tmpdir where local copies of clips are stored')
parser.add_argument('-o', '--outdir', type=str, required=True, help='Local path to directory where outputs will be dumped')
parser.add_argument('-n', '--batchname', type=str, required=True, help='Name of batch; will be printed in results to help organization between runs')
parser.add_argument('-k', type=int, required=True, help='Number of in-context examples being provided. Must be 0, 1, 2, or 3.')
parser.add_argument('-m', '--model', type=str, required=True, help='Name model to use. Options: "qwen", "internvl", "llava".')
args = parser.parse_args()
bfile, out_dir, tmp_dir, batch_name, k, model_name  = args.benchmark, args.outdir, args.tmpdir, args.batchname, args.k, args.model.lower()
if not os.path.isfile(bfile):
    raise Exception('ERROR: provided benchmark file could not be located.')
if not os.path.isdir(tmp_dir):
    raise Exception('ERROR: tmpdir must exist already and be populated with videos.')
if k not in set([0, 1, 2, 3]):
    raise Exception('ERROR: the `-k` flag must be set to 0, 1, 2, or 3.')
k_dict = {0: '', 1: 'video shows', 2: 'two videos show', 3: 'three videos show'}
k_text = k_dict[k]
them_or_it = 'them' if k > 1 else 'it'

# load in benchmark
with open(bfile, 'r') as f:
    benchmark = json.load(f)
action_ids = list(benchmark.keys())

# Get list of videos in tmp_dir and map UUIDs to them
uuid_to_filename: dict[str, str] = {}
tmp_dir_files = os.listdir(tmp_dir)
for tmp_dir_file in tmp_dir_files:
    parts = tmp_dir_file.split('.')
    if len(parts) != 2:
        raise Exception(f'The file {tmp_dir_file} contains an undesired period in the filename.')
    unique, _ = parts
    uuid_to_filename[unique] = tmp_dir_file

# While we're at it, get their absolute paths too
uuid_to_abspath: dict[str, str] = {}
for unique in uuid_to_filename.keys():
    filename = uuid_to_filename[unique]
    uuid_to_abspath[unique] = os.path.abspath(os.path.join(tmp_dir, filename))
    
# Shared utilities
def extract_prediction(text: str) -> str:
    pred = text.splitlines()[-1].strip().lower()
    pred = pred.replace('*', '').replace('#', '').replace(',', '').replace('.', '').replace(':', '')
    parts = pred.split(' ')
    if pred == "yes" or pred == "no": return pred
    if len(parts) > 1:
        first, last = parts[0], parts[-1]
        if first == 'yes' or first == 'no': return first
        elif last == 'yes' or last == 'no': return last
    if "boxed{yes}" in pred: return "yes"
    if "boxed{no}" in pred: return "no"
    if "does not show" in pred or "does not depict" in pred: return "no"
    if "the answer is yes" in pred: return "yes"
    if 'is "yes"' in pred: return "yes"
    if 'is "no"' in pred: return "no"
    return pred
        
def check_correct(prediction: str, expected: bool) -> int:
    pred = prediction.strip().lower()
    if expected:
        return 1 if pred == 'yes' else (0 if pred == 'no' else -1)
    else:
        return 1 if pred == 'no' else (0 if pred == 'yes' else -1)
    
def check_output(output: str, expected: bool) -> int:
    txt = output.replace(',', '').replace(';', '').replace(':', '').replace('.', '').replace('*', '').replace('#', '').lower()
    parts = txt.lower().split()
    start, end = parts[0], parts[-1]
    
    if start == 'yes' or end == 'yes' or "answer: yes" in txt or 'the answer is "yes' in txt or 'the answer is yes' in txt: 
        return 1 if expected else 0
    
    if start == 'no' or end == 'no' or "answer: no" in txt or 'answer is "no' in txt or 'answer is no' in txt:
        return 0 if expected else 1
    
    return -1

# Initialize models and prepare caches/inputs as needed
if model_name == 'qwen':
    print('Using Qwen 2.5 VL via HF Transformers.')
    QWEN_FPS = 2.0
    
    from transformers import Qwen2_5_VLForConditionalGeneration
    from qwen_vl_utils import process_vision_info
    
    os.environ['DECORD_DUPLICATE_WARNING_THRESHOLD'] = '1.0'
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        "Qwen/Qwen2.5-VL-7B-Instruct", torch_dtype=torch.bfloat16, device_map="auto",
        attn_implementation="flash_attention_2", cache_dir=MODELS_DIR
    )
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct", use_fast=True)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
elif model_name == 'internvl':
    print('Using Intern VL 3.')
    MAX_INTERNVL_FRAMES = 64
    
    IMAGENET_MEAN = (0.485, 0.456, 0.406)
    IMAGENET_STD = (0.229, 0.224, 0.225)
    
    model = AutoModel.from_pretrained(
        "OpenGVLab/InternVL3-8B", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True,
        use_flash_attn=True, trust_remote_code=True, cache_dir=MODELS_DIR
    ).eval().cuda()
    tokenizer = AutoTokenizer.from_pretrained("OpenGVLab/InternVL3-8B", trust_remote_code=True, use_fast=False)
    generation_config = dict(max_new_tokens=12000, do_sample=False, temperature=0)
    
    def build_transform(input_size):
        MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
        transform = T.Compose([
            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
            T.ToTensor(),
            T.Normalize(mean=MEAN, std=STD)
        ])
        return transform
    
    def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
        best_ratio_diff = float('inf')
        best_ratio = (1, 1)
        area = width * height
        for ratio in target_ratios:
            target_aspect_ratio = ratio[0] / ratio[1]
            ratio_diff = abs(aspect_ratio - target_aspect_ratio)
            if ratio_diff < best_ratio_diff:
                best_ratio_diff = ratio_diff
                best_ratio = ratio
            elif ratio_diff == best_ratio_diff:
                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                    best_ratio = ratio
        return best_ratio

    def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
        orig_width, orig_height = image.size
        aspect_ratio = orig_width / orig_height

        # calculate the existing image aspect ratio
        target_ratios = set(
            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
            i * j <= max_num and i * j >= min_num)
        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

        # find the closest aspect ratio to the target
        target_aspect_ratio = find_closest_aspect_ratio(
            aspect_ratio, target_ratios, orig_width, orig_height, image_size)

        # calculate the target width and height
        target_width = image_size * target_aspect_ratio[0]
        target_height = image_size * target_aspect_ratio[1]
        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

        # resize the image
        resized_img = image.resize((target_width, target_height))
        processed_images = []
        for i in range(blocks):
            box = (
                (i % (target_width // image_size)) * image_size,
                (i // (target_width // image_size)) * image_size,
                ((i % (target_width // image_size)) + 1) * image_size,
                ((i // (target_width // image_size)) + 1) * image_size
            )
            # split the image
            split_img = resized_img.crop(box)
            processed_images.append(split_img)
        assert len(processed_images) == blocks
        if use_thumbnail and len(processed_images) != 1:
            thumbnail_img = image.resize((image_size, image_size))
            processed_images.append(thumbnail_img)
        return processed_images

    def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):
        if bound:
            start, end = bound[0], bound[1]
        else:
            start, end = -100000, 100000
        start_idx = max(first_idx, round(start * fps))
        end_idx = min(round(end * fps), max_frame)
        seg_size = float(end_idx - start_idx) / num_segments
        frame_indices = np.array([
            int(start_idx + (seg_size / 2) + np.round(seg_size * idx))
            for idx in range(num_segments)
        ])
        return frame_indices
    
    def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=2):
        vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)
        max_frame = len(vr) - 1
        fps = float(vr.get_avg_fps())
        # duration = len(vr) / fps
        # num_segments = math.ceil(duration * sample_rate)

        pixel_values_list, num_patches_list = [], []
        transform = build_transform(input_size=input_size)
        frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)
        for frame_index in frame_indices:
            img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')
            img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)
            pixel_values = [transform(tile) for tile in img]
            pixel_values = torch.stack(pixel_values)            # [1, 3, input_size, input_size] torch Tensor
            num_patches_list.append(pixel_values.shape[0])
            pixel_values_list.append(pixel_values)
        pixel_values = torch.cat(pixel_values_list)
        return pixel_values, num_patches_list

elif model_name == 'llava':
    print('Using LLaVA-Video via the LLaVA-NeXT codebase.')
    LLAVA_MAX_FRAMES_TOTAL = 110
    CONV_TEMPLATE = 'qwen_2'
    
    # llava-specific imports
    from llava.model.builder import load_pretrained_model
    from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token
    from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX
    from llava.conversation import conv_templates, SeparatorStyle
    import warnings, copy
    warnings.filterwarnings("ignore")
    
    # load in model
    pretrained = "lmms-lab/LLaVA-Video-7B-Qwen2"
    device = "cuda"
    device_map = "auto"
    tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, 'llava_qwen', torch_dtype="bfloat16", device_map=device_map, cache_dir=MODELS_DIR)
    model.eval()
    
    # utils
    def load_video(video_path, max_frames_num,fps=1,force_sample=False):
        if max_frames_num == 0:
            return np.zeros((1, 336, 336, 3))
        vr = VideoReader(video_path, ctx=cpu(0),num_threads=1)
        total_frame_num = len(vr)
        video_time = total_frame_num / vr.get_avg_fps()
        fps = round(vr.get_avg_fps()/fps)
        frame_idx = [i for i in range(0, len(vr), fps)]
        frame_time = [i/fps for i in frame_idx]
        if len(frame_idx) > max_frames_num or force_sample:
            sample_fps = max_frames_num
            uniform_sampled_frames = np.linspace(0, total_frame_num - 1, sample_fps, dtype=int)
            frame_idx = uniform_sampled_frames.tolist()
            frame_time = [i/vr.get_avg_fps() for i in frame_idx]
        frame_time = ",".join([f"{i:.2f}s" for i in frame_time])
        spare_frames = vr.get_batch(frame_idx).asnumpy()
        return spare_frames,frame_time,video_time
    
    def prepare_conv(question: str) -> str:
        conv = copy.deepcopy(conv_templates[CONV_TEMPLATE])
        # conv.system = '<|im_start|>system\nYou are a helpful assistant who always outputs your thought process before providing any answer.'
        conv.append_message(conv.roles[0], question)
        conv.append_message(conv.roles[1], None)
        prompt_question = conv.get_prompt()
        return prompt_question

else:
    print("ERROR: the open-source model specified via the `-m` flag is not supported.")
    exit(1)

# Abstraction for model
if model_name == 'qwen':    
    def prep_binary(a_aan, action, s_aan, subdomain, domain, in_context_uuids: list[str], test_clip_uuid: str) -> tuple[list[dict[str, Any]]]:
        in_context_content = [{"type": "video", "video": f"file://{uuid_to_abspath[unique]}", "fps": QWEN_FPS} for unique in in_context_uuids]
        test_clip_content = {"type": "video", "video": f"file://{uuid_to_abspath[test_clip_uuid]}", "fps": QWEN_FPS}
        
        if k == 0:
            message = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f'Recall that {a_aan} {action} is {s_aan} {subdomain} in {domain}. Does the following video show {a_aan} {action}? It is critical that you first provide detailed reasoning behind your choice, and then output "yes" or "no" on the final line of your answer.'
                        },
                        test_clip_content
                    ]
                }
            ]
        else:
            message = [
                {
                    "role": "user", 
                    "content": [
                        {
                            "type": "text", 
                            "text": f"The following {k_text} {a_aan} {action}, which is {s_aan} {subdomain} in {domain}. Examine {them_or_it} closely.\n"
                        },
                        *in_context_content,
                        {
                            "type": "text", 
                            "text": f'Now consider the following video. Is it also {a_aan} {action}? It is critical that you first provide detailed reasoning behind your choice. It is also critical that the final line of your answer is exactly "yes" or "no".'
                        },
                        test_clip_content
                    ]
                }
            ]

        return message,     # must return tuple so it can be unpacked when sending into inference
    
    def inference(message: list[dict[str, Any]]) -> str:
        messages = message
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt")
        inputs = inputs.to(device)

        generated_ids = model.generate(**inputs, max_new_tokens=128000, do_sample=False, temperature=0)
        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
        output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False, do_rescale=False)

        return output_text[0]

elif model_name == 'internvl':
    def process_videos(vid_paths: list[str], frames_per_video: int) -> tuple[str, torch.Tensor, list[int]]:
        combined_pixel_values: list[torch.Tensor] = []
        combined_num_patches_list: list[int] = []
        for vid_path in vid_paths:
            curr_pixel_values, curr_num_patches_list = load_video(vid_path, max_num=1, num_segments=frames_per_video)
            curr_pixel_values = curr_pixel_values.to(torch.bfloat16).cuda()
            combined_pixel_values.append(curr_pixel_values)
            combined_num_patches_list.extend(curr_num_patches_list)
        return torch.cat(combined_pixel_values, dim=0), combined_num_patches_list
    
    def prep_binary(a_aan, action, s_aan, subdomain, domain, in_context_uuids: list[str], test_clip_uuid: str) -> tuple[str, torch.Tensor, list[int]]:
        if len(in_context_uuids) != k: raise RuntimeError('The amount of in-context examples provided did not equal the integer provided via the `-k` flag.')
        num_videos = len(in_context_uuids) + 1
        frames_per_video = MAX_INTERNVL_FRAMES // num_videos
        vid_paths = [uuid_to_abspath[u] for u in in_context_uuids + [test_clip_uuid]]
        pixel_values, num_patches_list = process_videos(vid_paths, frames_per_video)
        video_prefixes = [''.join([f'Frame{j+1}: <image>\n' for j in range(i*frames_per_video,(i+1)*frames_per_video)]) for i in range(num_videos)]
        
        if k == 0:
            question = ''.join([
                f'Recall that {a_aan} {action} is {s_aan} {subdomain} in {domain}. Does the following video show {a_aan} {action}? Please reason through your answer. It is critical that you output "yes" or "no" on the final line of your answer.\n',
                video_prefixes[0]
            ])
        else:
            in_context_content = []
            for i in range(k): in_context_content.extend([f'\nVideo{i+1}:\n', video_prefixes[i]])
            question = ''.join([
                f"The following {k_text} {a_aan} {action}, which is {s_aan} {subdomain} in {domain}. Examine {them_or_it} closely.\n",
                *in_context_content,
                f'\nNow consider the following video. Does it also show {a_aan} {action}? It is critical that you first provide detailed reasoning behind your choice, and then output exactly "yes" or "no" on the final line of your answer.\n',
                f'\nVideo{num_videos}:\n', video_prefixes[-1]
            ])
        
        return question, pixel_values, num_patches_list
    
    def inference(question: str, pixel_values: torch.Tensor, num_patches_list: list[int]) -> str:
        response, history = model.chat(tokenizer, pixel_values, question, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)
        return response

elif model_name == 'llava':
    def process_videos(video_paths: list[str]) -> tuple[list[torch.Tensor, str, float], list[torch.Tensor]]:
        max_frames_per_video = LLAVA_MAX_FRAMES_TOTAL // len(video_paths)
        
        video_infos: list[torch.Tensor, str, float] = []
        video_inputs: list[torch.Tensor] = []
        for vid_path in video_paths:
            video, frame_time, video_time = load_video(vid_path, max_frames_per_video, 1, force_sample=True)
            video = image_processor.preprocess(video, return_tensors="pt")["pixel_values"].cuda().bfloat16()
            video_infos.append((video, frame_time, video_time))  
            video_inputs.append(video)
        return video_infos, video_inputs
        
    def prep_binary(a_aan, action, s_aan, subdomain, domain, in_context_uuids: list[str], test_clip_uuid: str) -> tuple[str, list[torch.Tensor]]:
        all_uuids: list[str] = in_context_uuids + [test_clip_uuid]
        video_paths = [uuid_to_abspath[u] for u in all_uuids]
        video_infos, video_inputs = process_videos(video_paths)
        
        if k == 0:
            video, frame_time, video_time = video_infos[0]
            question = "".join([
                DEFAULT_IMAGE_TOKEN + f'The video lasts for {video_time:.2f} seconds, and {len(video)} frames are uniformly sampled from it. These frames are located at {frame_time}. Please answer the following questions related to this video.\n',
                f'Recall that {a_aan} {action} is {s_aan} {subdomain} in {domain}. Does this video show {a_aan} {action}? Please reason through your answer. It is critical that you output "yes" or "no" on the final line of your answer.'
            ])
        else:
            k_dict = {1: 'video shows', 2: 'two videos each show', 3: 'three videos each show'}
            k_text = k_dict[k]
            question = f"The following {k_text} {a_aan} {action}, which is {s_aan} {subdomain} in {domain}. Examine {them_or_it} closely.\n"
            for i in range(len(video_infos) - 1):
                vid_info = video_infos[i]
                video, frame_time, video_time = vid_info
                question += f"VIDEO #{i+1}: {DEFAULT_IMAGE_TOKEN}The video lasts for {video_time:.2f} seconds, and {len(video)} frames are uniformly sampled from it. These frames are located at {frame_time}.\n"
                
            question += f'\nNow consider the following video. Does it also show {a_aan} {action}? Please reason through your answer. It is critical that you output "yes" or "no" on the final line of your answer.\n'
            video, frame_time, video_time = video_infos[-1]
            question += f"VIDEO #{len(video_paths)}: {DEFAULT_IMAGE_TOKEN}The video lasts for {video_time:.2f} seconds, and {len(video)} frames are uniformly sampled from it. These frames are located at {frame_time}.\n"
            
        prompt_question: str = prepare_conv(question)
        
        return prompt_question, video_inputs
        
    def inference(prompt_question: str, video_inputs: list[torch.Tensor]) -> str:
        input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt").unsqueeze(0).to(device)
        cont = model.generate(
            input_ids,
            images=video_inputs,
            modalities= ["video"] * len(video_inputs),
            do_sample=False,
            temperature=0,
            max_new_tokens=32000,
        )
        text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)[0].strip()
        return text_outputs

else:
    # defensive programming; should never be reached
    raise Exception(f'ERROR: the open-source model specified via the `-m` flag is not supported. FYI, you supplied {model_name}.')

# Core evaluation logic
errors: list[str] = []
results: dict[str, dict] = {}
for id in tqdm(action_ids, desc='Running benchmark'):
    # unpack JSON entry
    tmp = benchmark[id]
    action, domain, subdomain = tmp['action'], tmp['domain'], tmp['subdomain']
    subdomain = tmp['subdomain'] if tmp['subdomain'] and tmp['subdomain'] != 'NULL' else 'action'
    a_aan = 'an' if action[0].lower() in set(['a', 'i', 'o', 'u', 'e']) else 'a'
    s_aan = 'an' if subdomain[0].lower() in set(['a', 'i', 'o', 'u', 'e']) else 'a'
    out = {}
    
    # gather in-context examples and positive clips
    in_context, positives = tmp['in_context'], tmp['positives']
    
    # run positive clips through model
    for i in range(len(positives)):
        try:
            prepared_inputs: tuple = prep_binary(a_aan, action, s_aan, subdomain, domain, in_context, positives[i])
            output: str = inference(*prepared_inputs)
            prediction: str = extract_prediction(output)
            correct: int = check_correct(prediction, True)
            if correct == -1: correct = check_output(output, True)
            out[f'pos{i+1}'] = {'accurate': correct, 'prediction': prediction, 'response': output}
            if correct == -1: 
                raise Exception('Could not parse the following (truncated) prediction: ', prediction[:100])
        except Exception as e:
            print('EXCEPTION:', e)
            errors.append(f'On UUID {positives[i]} as positive clip #{i+1} for action {action} (id: {id})')

    # gather negative clips
    neg_info = tmp['negatives']
    negatives = [n['uuid'] for n in neg_info]
    
    # run negative clips through model
    for i in range(len(negatives)):
        try:
            prepared_inputs: tuple = prep_binary(a_aan, action, s_aan, subdomain, domain, in_context, negatives[i])
            output: str = inference(*prepared_inputs)
            prediction: str = extract_prediction(output)
            correct: int = check_correct(prediction, False)
            if correct == -1: correct = check_output(output, False)
            out[f'neg{i+1}'] = {'accurate': correct, 'prediction': prediction, 'response': output,
                                'neg_id': neg_info[i]['action_id'], 'neg_name': neg_info[i]['action_name']}
            if correct == -1: 
                raise Exception('Could not parse the following (truncated) prediction: ', prediction[:100])
        except Exception as e:
            print('EXCEPTION:', e)
            errors.append(f'On UUID {negatives[i]} as negative clip #{i+1} for action {action} (id: {id})')
    
    # save results
    results[id] = out
    
# Create a succint version of results for easy analysis
succint: dict[str, dict] = {}
pos_acc, neg_acc, pos_total, neg_total = 0, 0, 0, 0
for id in results.keys():
    result = results[id]
    pacc, nacc, ptotal, ntotal = 0, 0, 0, 0
    i = 1
    while f'pos{i}' in result or f'neg{i}' in result:
        if f'pos{i}' in result:
            ptotal += 1
            if result[f'pos{i}']['accurate']: pacc += 1
        if f'neg{i}' in result:
            ntotal += 1
            if result[f'neg{i}']['accurate']: nacc += 1
        i += 1
    succint[id] = {'acc': pacc + nacc, 'total': ptotal + ntotal, 
                   'pos_acc': pacc, 'pos_total': ptotal,
                   'neg_acc': nacc, 'neg_total': ntotal}
    pos_acc, neg_acc = pos_acc + pacc, neg_acc + nacc
    pos_total, neg_total = pos_total + ptotal, neg_total + ntotal

# Write outputs
os.makedirs(out_dir, exist_ok=True)
results_path, errors_path = os.path.join(out_dir, 'results.json'), os.path.join(out_dir, 'errors.txt')
succint_path, summary_path = os.path.join(out_dir, 'succint.json'), os.path.join(out_dir, 'summary.txt')

with open(results_path, 'w') as f:
    json.dump(results, f)
    f.write('\n')
with open(succint_path, 'w') as f:
    json.dump(succint, f)
    f.write('\n')
with open(errors_path, 'w') as f:
    errors_txt = "\t ERRORS \t" + "\n".join(errors)
    f.write(errors_txt)
    f.write('\n')
    
# Generate a text summary for even easier analysis
try:
    acc, total = pos_acc + neg_acc, pos_total + neg_total
    pos_rate = 100 * pos_acc / pos_total if pos_total else 0.
    neg_rate = 100 * neg_acc / neg_total if neg_total else 0.
    rate = 100 * acc / total if total else 0.
    summary = f"""\n\t ######## SUMMARY ######## \t\n
    \t Positive Clips : {pos_acc} / {pos_total} correct ( {pos_rate:.2f}% )
    \t Negative Clips : {neg_acc} / {neg_total} correct ( {neg_rate:.2f}% )
    \t Total          : {acc} / {total} correct ( {rate:.2f}% )

    \t Num Errors     : {len(errors)}

    \t Model          : {model_name}
    \t Batch          : {batch_name}
    \t Date           : {time.strftime("%Y-%m-%d %H:%M:%S %z")}
    \t Length         : {timedelta(seconds=int(time.time() - program_start))}
    \t Results Dir    : {out_dir}
    \t k-Shot         : {k}
    \n\t ######## END TRANSMISSION ######## \t\n"""
    with open(summary_path, 'w') as f:
        f.write(summary)
    print(summary)
except Exception as e:
    print("An error occured while trying to generate a summary.")
    print("EXCEPTION: ", e)