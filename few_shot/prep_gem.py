import argparse, os, json, subprocess, time, pytz
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from collections import deque
from threading import Lock
from copy import deepcopy
from google import genai
from tqdm import tqdm

parser = argparse.ArgumentParser()
parser.add_argument('-d', '--dlfile', type=str, required=True, help='Path to outfile generated by prep_benchmark.py')
parser.add_argument('-t', '--tmpdir', type=str, required=True, help='Temporary directory where videos are downloaded from GCS before uploading to Gemini File API')
parser.add_argument('-r', '--rfile', type=str, required=True, help='Path where JSON file with UUID --> file name mapping will be written.')
parser.add_argument('--hours', type=int, required=False, default=2, help='Videos that are currently on the Gemini File API but will expire in this many hours will be re-uploaded.')
args = parser.parse_args()
dlfile, rfile, tmp_dir, hours_cushion = args.dlfile, args.rfile, args.tmpdir, args.hours
if not os.path.isfile(dlfile): raise Exception('The provided downloads file (dlfile) does not exist.')
os.makedirs(tmp_dir, exist_ok=True)

# get list of clips (as identified by UUIDs) that must be in Gemini File API by program termination
with open(dlfile, 'r') as f:
    urls = json.load(f)
tails = [u.split('/')[-1] for u in urls]
uuids_list = [t.split('.')[0] for t in tails]
uuids = set(uuids_list)
if not (len(uuids) == len(uuids_list) == len(urls)):
    raise Exception('ERROR: duplicate UUIDs or misformatted URL.')
uuid_to_url_tail: dict[str, tuple[str, str]] = {uuids_list[i]: (urls[i], tails[i]) for i in range(len(uuids))}

# get api key
api_file = os.environ.get("GOOGLE_GENAI", "/gscratch/raivn/tanush/credentials/google_genai.txt")
try:
    with open(api_file, 'r') as f:
        api_key = f.read().strip()
except Exception as e:
    print("ERROR: unable to read Google GenAI API Key at ", api_file)
    raise e
client = genai.Client(api_key=api_key)

# get list of clips that are already in Gemini File API and will be there 2 hours from now
uuid_to_gfilename: dict[str, str] = {}
now = datetime.now() + timedelta(minutes=15)
existing_files = client.files.list()
files_list_copy = []
for f in existing_files:
    files_list_copy.append(deepcopy(f))
    unique = f.display_name
    # only care about about the files in the Gemini API that our benchmark uses -- let's not mess with other folks' files
    if unique in uuids:
        # if it's already uploaded & will not expire soon, let's avoid re-uploading it
        if f.state == genai.types.FileState.ACTIVE and f.expiration_time - pytz.UTC.localize(now).astimezone(f.expiration_time.tzinfo) > timedelta(hours=hours_cushion):
            uuid_to_gfilename[unique] = f.name
        # otherwise let's try deleting this file since it's likely corrupt (not active yet) OR it will expire soon
        else:
            try:
                client.files.delete(name=f.name)
            except:
                pass

to_upload = uuids.difference(set(uuid_to_gfilename.keys()))

print(f"\nThis benchmark requires {len(uuids)} files. {len(uuid_to_gfilename)} files are already in the Gemini File API. We will now upload the remaining {len(to_upload)} ones.\n")

# download the clips that must be uploaded to Gemini File API
vid_paths: dict[str, str] = {}
for unique in tqdm(uuids, desc="downloading clips"):
    url, tail = uuid_to_url_tail[unique]
    vid_path = os.path.join(tmp_dir, tail)
    vid_paths[unique] = vid_path
    if os.path.isfile(vid_path): continue
    gcs_url = "gs://" + url.split('https://storage.googleapis.com/')[-1]
    subprocess.run(['gcloud', 'storage', 'cp', gcs_url, vid_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# upload those clips to Gemini File API
uuid_lock = Lock()
not_active_lock = Lock()

#   core logic
not_active = []
def upload_clip(unique):
    vid_path = vid_paths[unique]
    file_obj = client.files.upload(file=vid_path, config={'display_name': unique})
    time.sleep(10)
    name = file_obj.name
    x = client.files.get(name=name)
    
    with uuid_lock:
        uuid_to_gfilename[unique] = name
    
    if x.state != genai.types.FileState.ACTIVE:
        time.sleep(20)
        with not_active_lock:
            not_active.append((name, unique))

#   parallelization logic
futures = []
with ThreadPoolExecutor(max_workers=8) as executor:
    for i, unique in enumerate(to_upload):
        if i > 0 and i % 8 == 0:
            time.sleep(30)
        futures.append(executor.submit(upload_clip, unique))
    for _ in tqdm(as_completed(futures), total=len(futures), desc="uploading clips"):
        pass

# Check on the files that hadn't uploading/processing earlier
if not_active:
    print(f"\nDone uploading files. {len(not_active)} of them are not in the ACTIVE state. Sleeping for fifteen minutes then will check on them.\n")
    time.sleep(900)
    
    not_active_2 = []
    for name, unique in tqdm(not_active, desc='checking non-active clips'):
        state = client.files.get(name=name).state 
        if state != genai.types.FileState.ACTIVE:
            # errors.append(f"ERROR: file with UUID {unique} and gfilename {name} has state {state}.")
            not_active_2.append((name, unique))
        time.sleep(2)
    
    if not_active_2:
        print(f"\n{len(not_active_2)} of them are **STILL** not in the ACTIVE state. Sleeping for fifteen minutes then will check on them.\n")
        time.sleep(900)
        
        not_active_3 = []
        for name, unique in tqdm(not_active_2, desc='checking non-active clips (pass 2)'):
            state = client.files.get(name=name).state 
            if state != genai.types.FileState.ACTIVE:
                not_active_3.append((name, unique))
            time.sleep(2)
    
        if not_active_3:
            print(f"\n{len(not_active_3)} of them are **STILL** not in the ACTIVE state. Deleting them then trying to re-upload.\n")
            
            reuploads = []
            for name, unique in tqdm(not_active_3, desc='reuploading non-active clips'):
                state = client.files.get(name=name).state
                if state == genai.types.FileState.ACTIVE:
                    continue
                client.files.delete(name=name)
                del uuid_to_gfilename[unique]
                file_obj = client.files.upload(file=vid_paths[unique])
                name = file_obj.name
                uuid_to_gfilename[unique] = name
                reuploads.append((name, unique))
            
            print(f"\nWaiting ten minutes then checking them.\n")
            time.sleep(600)
            errors = []
            for name, unique in tqdm(reuploads, desc='checking reuploaded clips (pass 3)'):
                state = client.files.get(name=name).state
                if state != genai.types.FileState.ACTIVE:
                    del uuid_to_gfilename[unique]
                    errors.append(f"ERROR: file with UUID {unique} and gfilename {name} has state {state}.")

            if errors:
                error_txt = "\n".join(errors)
                dirname, basename = os.path.dirname(rfile), os.path.basename(rfile)
                errors_file = os.path.join(dirname, f"errors_{basename.replace('.json', '.txt')}")
                print(error_txt + '\n')
                with open(errors_file, 'w') as f:
                    f.write(error_txt)
                    f.write('\n')

# write UUID to name (as in file name for Gemini File API) to output file for use in `eval_gem.py`
with open(rfile, 'w') as f:
    json.dump(uuid_to_gfilename, f)